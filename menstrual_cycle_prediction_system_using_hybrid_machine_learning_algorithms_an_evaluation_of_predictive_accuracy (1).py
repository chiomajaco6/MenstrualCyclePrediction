# -*- coding: utf-8 -*-
"""Menstrual Cycle Prediction System using Hybrid Machine Learning Algorithms: An Evaluation of Predictive Accuracy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1USp-Bi1WielCW4qtK7Zc2ILGALhbsO93

# Decision Tree
"""

!pip install scikit-learn

#Import the necessary librabries
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, recall_score, r2_score
import matplotlib.pyplot as plt
import seaborn as sn

#Import the dataset
df = pd.read_csv("/content/FedCycleData071012 (2).csv")

#To view five rows out of the dataset
df.head()

# Data preprocessing
# Select relevant features and target variable (e.g., 'EstimatedDayofOvulation' LengthofLutealPhase,
# CycleNumber: To track the individual's cycle history.
# LengthofCycle: The length of the menstrual cycle.
# LengthofLutealPhase: The length of the luteal phase.
# TotalNumberofHighDays: The number of high fertility days.
# TotalNumberofPeakDays: The number of peak fertility days.
# UnusualBleeding: Information about unusual bleeding patterns.
# PhasesBleeding: Different phases of bleeding during the cycle.
# IntercourseInFertileWindow: Whether intercourse occurred during the fertile window.
# Age: Age of the individual.
# BMI: Body Mass Index.
# Method: Information about contraceptive methods used.)
ovulation_dataset = df[['CycleNumber', 'LengthofCycle', 'LengthofLutealPhase', 'TotalNumberofHighDays', 'TotalNumberofPeakDays', 'UnusualBleeding', 'PhasesBleeding', 'IntercourseInFertileWindow','Age','BMI','Method','EstimatedDayofOvulation']]

#To determine the shape of the dataset
ovulation_dataset.shape

ovulation_dataset.head(100)

ovulation_dataset.duplicated().sum()
ovulation_dataset.drop_duplicates(inplace=True)

ovulation_dataset.columns

# 1. Replacing missing values(Nan) in CycleNumber with the mode
mode_CycleNumber = ovulation_dataset['CycleNumber'].mode()[0]
ovulation_dataset['CycleNumber'].fillna(mode_CycleNumber, inplace=True)

# 2. Replacing missing values(Nan) in LengthofCycle with the mode
mode_LengthofCycle = ovulation_dataset['LengthofCycle'].mode()[0]
ovulation_dataset['LengthofCycle'].fillna(mode_LengthofCycle, inplace=True)

# 3. Replacing missing values(Nan) in LengthofLutealPhase with the mode
mode_LengthofLutealPhase = ovulation_dataset['LengthofLutealPhase'].mode()[0]
ovulation_dataset['LengthofLutealPhase'].fillna(mode_LengthofLutealPhase, inplace=True)

# 4. Replacing missing values(Nan) in TotalNumberofHighDays with the mode
mode_TotalNumberofHighDays = ovulation_dataset['TotalNumberofHighDays'].mode()[0]
ovulation_dataset['TotalNumberofHighDays'].fillna(mode_TotalNumberofHighDays, inplace=True)

# 5. Replacing missing values(Nan) in TotalNumberofPeakDays with the mode
mode_TotalNumberofPeakDays = ovulation_dataset['TotalNumberofPeakDays'].mode()[0]
ovulation_dataset['TotalNumberofPeakDays'].fillna(mode_TotalNumberofPeakDays, inplace=True)

# 6. Replacing missing values(Nan) in UnusualBleedingwith the mode
mode_UnusualBleeding = ovulation_dataset['UnusualBleeding'].mode()[0]
ovulation_dataset['UnusualBleeding'].fillna(mode_UnusualBleeding, inplace=True)

# 7. Replacing missing values(Nan) in PhasesBleeding  with the mode
mode_PhasesBleeding  = ovulation_dataset['PhasesBleeding'].mode()[0]
ovulation_dataset['PhasesBleeding'].fillna(mode_PhasesBleeding  , inplace=True)

# 8. Replacing missing values(Nan) in PhasesBleeding  with the mode
mode_IntercourseInFertileWindow  = ovulation_dataset['IntercourseInFertileWindow'].mode()[0]
ovulation_dataset['IntercourseInFertileWindow'].fillna(mode_IntercourseInFertileWindow, inplace=True)

# 9. Replacing missing values(Nan) in Age with the mode
mode_Age    = ovulation_dataset['Age'].mode()[0]
ovulation_dataset['Age'].fillna(mode_Age , inplace=True)

# 10. Replacing missing values(Nan) in BMI   with the mode
mode_BMI = ovulation_dataset['BMI'].mode()[0]
ovulation_dataset['BMI'].fillna(mode_BMI, inplace=True)

# 11. Replacing missing values(Nan) in Method  with the mode
mode_Method = ovulation_dataset['Method'].mode()[0]
ovulation_dataset['Method'].fillna(mode_Method  , inplace=True)

# 12. Replacing missing values(Nan) in EstimatedDayofOvulation with the mode
mode_EstimatedDayofOvulation = df['EstimatedDayofOvulation'].mode()[0]
ovulation_dataset['EstimatedDayofOvulation'].fillna(mode_EstimatedDayofOvulation, inplace=True)

# Clean the 'EstimatedDayofOvulation' column by replacing empty strings with NaN and converting to float
ovulation_dataset['EstimatedDayofOvulation'] = pd.to_numeric(df['EstimatedDayofOvulation'], errors='coerce')

# Clean the 'LengthofLutealPhase  ' column by replacing empty strings with NaN and converting to float
ovulation_dataset['LengthofLutealPhase'] = pd.to_numeric(df['LengthofLutealPhase'], errors='coerce')

# Clean the 'LengthofCycle' column by replacing empty strings with NaN and converting to float
ovulation_dataset['LengthofCycle'] = pd.to_numeric(df['LengthofCycle'], errors='coerce')

ovulation_dataset.isnull().sum()

#To determine the datatype and the data columns of the dataset
ovulation_dataset.info()

ovulation_dataset.head(5)

# Convert the column to int
ovulation_dataset['CycleNumber'] = ovulation_dataset['CycleNumber'].astype(int)
ovulation_dataset['LengthofCycle'] = ovulation_dataset['LengthofCycle'].astype(int)
ovulation_dataset['LengthofLutealPhase'] = pd.to_numeric(ovulation_dataset['LengthofLutealPhase'], errors='coerce')
ovulation_dataset['LengthofLutealPhase'] = ovulation_dataset['LengthofLutealPhase'].fillna(0).astype(int)
ovulation_dataset['TotalNumberofHighDays'] = pd.to_numeric(ovulation_dataset['TotalNumberofHighDays'], errors='coerce')
ovulation_dataset['TotalNumberofHighDays'] = ovulation_dataset['TotalNumberofHighDays'].fillna(0).astype(int)
ovulation_dataset['TotalNumberofHighDays'] = pd.to_numeric(ovulation_dataset['TotalNumberofHighDays'], errors='coerce')
ovulation_dataset['TotalNumberofHighDays'] = ovulation_dataset['TotalNumberofHighDays'].fillna(0).astype(int)
ovulation_dataset['UnusualBleeding'] = pd.to_numeric(ovulation_dataset['UnusualBleeding'], errors='coerce')
ovulation_dataset['UnusualBleeding'] = ovulation_dataset['UnusualBleeding'].fillna(0).astype(int)
ovulation_dataset['PhasesBleeding'] = pd.to_numeric(ovulation_dataset['PhasesBleeding'], errors='coerce')
ovulation_dataset['PhasesBleeding'] = ovulation_dataset['PhasesBleeding'].fillna(0).astype(int)
ovulation_dataset['IntercourseInFertileWindow'] = pd.to_numeric(ovulation_dataset['IntercourseInFertileWindow'], errors='coerce')
ovulation_dataset['IntercourseInFertileWindow'] = ovulation_dataset['IntercourseInFertileWindow'].fillna(0).astype(int)
ovulation_dataset['Age'] = pd.to_numeric(ovulation_dataset['Age'], errors='coerce')
ovulation_dataset['Age'] = ovulation_dataset['Age'].fillna(0).astype(int)
ovulation_dataset['BMI'] = pd.to_numeric(ovulation_dataset['BMI'], errors='coerce')
ovulation_dataset['BMI'] = ovulation_dataset['BMI'].fillna(0).astype(int)
ovulation_dataset['Method'] = pd.to_numeric(ovulation_dataset['Method'], errors='coerce')
ovulation_dataset['Method'] = ovulation_dataset['Method'].fillna(0).astype(int)
ovulation_dataset['EstimatedDayofOvulation'] = pd.to_numeric(ovulation_dataset['EstimatedDayofOvulation'], errors='coerce')
ovulation_dataset['EstimatedDayofOvulation'] = ovulation_dataset['EstimatedDayofOvulation'].fillna(0).astype(int)

#Splitting the dataset
'''After preprocessing the dataset, it was then splitted into features and target set.
X represent the features dataset, while Y represent the target dataset.
The drop() function used here is used to drop the 'EstimatedDayofOvulation' from the dataset so as to separate it from the features dataset.
It is therefore shown below:'''
X = ovulation_dataset.drop('EstimatedDayofOvulation', axis= 1)
y = ovulation_dataset['EstimatedDayofOvulation']

#To view the values of X dataset
#The X represent the features dataset. It is shown by just typing X as shown in the figure below:
X



y

#Split the dataset into training and testing set
'''The 'X_train' represent the 85% of the training dataset. The 'X_test' represent the 15% for testing the model.
The 'y_train' represent the 85% of the target dataset that was used to train the model.
The 'y_test' represent the 15% of the target dataset that was used to test the model.'''

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15, random_state = 42)

#To develop the model the following was carried out:
from sklearn.tree import DecisionTreeRegressor
#Create an instance of the Decision Tree model
decision_tree = DecisionTreeRegressor()

X_train = X_train.dropna()
y_train = y_train.loc[X_train.index]

# Check the data types of X_train and y_train
x_train_data_types = X_train.dtypes
y_train_data_type = y_train.dtype

# Check if all elements in X_train are of float data type
x_train_float_check = all(x_train_data_types == 'float64')

# Check if y_train is of float data type
y_train_float_check = (y_train_data_type == 'float64')

# Print the results
print(f'X_train is of float data type: {x_train_float_check}')
print(f'y_train is of float data type: {y_train_float_check}')

from sklearn.preprocessing import LabelEncoder

# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Loop through columns in X_train
for column in X_train.columns:
    if X_train[column].dtype == 'object':  # Check if the column contains strings
        X_train[column] = label_encoder.fit_transform(X_train[column].astype(str))

# Convert y_train to float (assuming it's a target variable)
y_train = y_train.astype(float)

from sklearn.preprocessing import LabelEncoder

# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Loop through columns in X_test
for column in X_test.columns:
    if X_test[column].dtype == 'object':  # Check if the column contains strings
        X_test[column] = label_encoder.fit_transform(X_test[column].astype(str))

# Convert y_test to float (assuming it's a target variable)
y_test = y_test.astype(float)

#Train the model
decision_tree.fit(X_train, y_train)

from sklearn.metrics import r2_score
# Now, you can proceed with making predictions
y_pred_DT = decision_tree.predict(X_test)

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error, explained_variance_score, median_absolute_error
# Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred_DT )
print("Mean Squared Error for Decision Tree Algorithm:", mse)

# Root Mean Squared Error (RMSE)
rmse = mean_squared_error(y_test, y_pred_DT, squared=False)
print("Root Mean Squared Error for Decision Tree Algorithm:", rmse)

# Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y_pred_DT)
print("Mean Absolute Error for Decision Tree:", mae)

# R-squared (R2)
r2 = r2_score(y_test, y_pred_DT)
print("R-squared for Decision Tree Algorithm:", r2)

# Mean Absolute Percentage Error (MAPE)
mape = mean_absolute_percentage_error(y_test, y_pred_DT)
print("Mean Absolute Percentage Error for Decision Tree Algorithm:", mape)

# Explained Variance Score
explained_variance = explained_variance_score(y_test, y_pred_DT)
print("Explained Variance Score for Decision Tree Algorithm:", explained_variance)

# Median Absolute Error (MedAE)
medae = median_absolute_error(y_test, y_pred_DT)
print("Median Absolute Error for Decision Tree Algorithm:", medae)

import sklearn
print(sklearn.__version__)

#Picke the model
import pickle
import joblib
pd.to_pickle(decision_tree,  "/content/DecisionTreeForOvulationDayPrediction.pkl")
filename= 'DecisionTreeForOvulationDayPrediction.pkl'
saved_model= joblib.dump(decision_tree, filename)

import joblib

# Replace 'model_with_highest_r2.pkl' with the desired file name
model_filename = 'DecisionTreeRegressor.pkl'

# Replace 'model_with_highest_r2' with the variable name of your model
model_to_pickle = decision_tree

# Pickle the model using joblib
joblib.dump(model_to_pickle, model_filename)

"""# Random Forest Model

"""

#Split the dataset into training and testing set
'''The 'X_train' represent the 85% of the training dataset. The 'X_test' represent the 15% for testing the model.
The 'y_train' represent the 85% of the target dataset that was used to train the model.
The 'y_test' represent the 15% of the target dataset that was used to test the model.'''

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15, random_state = 42)

from sklearn.ensemble import RandomForestClassifier

# Create an instance of the Random Forest
#rf = RandomForestClassifier()
# Initialize the Random Forest classifier
rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)

from sklearn.preprocessing import LabelEncoder

# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Loop through columns in X_train
for column in X_train.columns:
    if X_train[column].dtype == 'object':  # Check if the column contains strings
        X_train[column] = label_encoder.fit_transform(X_train[column].astype(str))

# Convert y_train to float (assuming it's a target variable)
y_train = y_train.astype(float)

from sklearn.preprocessing import LabelEncoder

# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Loop through columns in X_test
for column in X_train.columns:
    if X_test[column].dtype == 'object':  # Check if the column contains strings
        X_test[column] = label_encoder.fit_transform(X_test[column].astype(str))

# Convert y_test to float (assuming it's a target variable)
y_test = y_test.astype(float)

# Train the model
rf_model.fit(X_train, y_train)

y_pred_RF = rf_model.predict(X_test)

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error, explained_variance_score, median_absolute_error
# Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred_RF)
print("Mean Squared Error for Random Forest Algorithm:", mse)

# Root Mean Squared Error (RMSE)
rmse = mean_squared_error(y_test, y_pred_RF, squared=False)
print("Root Mean Squared Error for Random Forest Algorithm:", rmse)

# Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y_pred_RF)
print("Mean Absolute Error for Random Forest Algorithm:", mae)

# R-squared (R2)
r2 = r2_score(y_test, y_pred_RF)
print("R-squared for Random Forest Algorithm:", r2)

# Mean Absolute Percentage Error (MAPE)
mape = mean_absolute_percentage_error(y_test, y_pred_RF)
print("Mean Absolute Percentage Error for Random Forest Algorithm:", mape)

# Explained Variance Score
explained_variance = explained_variance_score(y_test, y_pred_RF)
print("Explained Variance Score for Random Forest Algorithm:", explained_variance)

# Median Absolute Error (MedAE)
medae = median_absolute_error(y_test, y_pred_RF)
print("Median Absolute Error for Random Forest Algorithm:", medae)

"""# Linear Regression model

"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Create and train the Linear Regression model
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

# Make predictions and evaluate the model's performance
y_pred_LR = lr_model.predict(X_test)

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error, explained_variance_score, median_absolute_error
# Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred_LR)
print("Mean Squared Error for Linear Regression:", mse)

# Root Mean Squared Error (RMSE)
rmse = mean_squared_error(y_test, y_pred_LR, squared=False)
print("Root Mean Squared Error for Linear Regression:", rmse)

# Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y_pred_LR )
print("Mean Absolute Error for Linear Regression:", mae)

# R-squared (R2)
r2 = r2_score(y_test, y_pred_LR )
print("R-squared for Linear Regression:", r2)

# Mean Absolute Percentage Error (MAPE)
mape = mean_absolute_percentage_error(y_test, y_pred_LR )
print("Mean Absolute Percentage Error for Linear Regression:", mape)

# Explained Variance Score
explained_variance = explained_variance_score(y_test, y_pred_LR)
print("Explained Variance Score for Linear Regression:", explained_variance)

# Median Absolute Error (MedAE)
medae = median_absolute_error(y_test, y_pred_LR)
print("Median Absolute Error for Linear Regression:", medae)

"""# Comparision of the accuracy of the three models"""

import matplotlib.pyplot as plt

models = ['Decision Tree Regression', 'Random Forest', 'Logistic Regression']
r2_scores = [0.58, 0.35, 0.033]  # Example R2 scores

plt.bar(models, r2_scores)

plt.xlabel('Models')
plt.ylabel('R2 Score')
plt.title('Comparison of R2 Scores')

plt.show()

"""# Huber Regressor model"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import HuberRegressor
from sklearn.metrics import mean_squared_error, r2_score

# This section for Huber Regressor
huber_reg = HuberRegressor(epsilon=1.35)  # You can adjust the epsilon parameter
huber_reg.fit(X_train, y_train)

# Make predictions on the test set
y_pred_HuberReg = huber_reg.predict(X_test)

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error, explained_variance_score, median_absolute_error
# Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred_HuberReg)
print("Mean Squared Error for Huber Regressor:", mse)

# Root Mean Squared Error (RMSE)
rmse = mean_squared_error(y_test, y_pred_HuberReg, squared=False)
print("Root Mean Squared Error for Huber Regressor:", rmse)

# Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y_pred_HuberReg )
print("Mean Absolute Error for Huber Regressor:", mae)

# R-squared (R2)
r2 = r2_score(y_test, y_pred_HuberReg)
print("R-squared for Huber Regressor:", r2)

# Mean Absolute Percentage Error (MAPE)
mape = mean_absolute_percentage_error(y_test, y_pred_HuberReg)
print("Mean Absolute Percentage Error for Huber Regressor:", mape)

# Explained Variance Score
explained_variance = explained_variance_score(y_test, y_pred_HuberReg)
print("Explained Variance Score for Huber Regressor:", explained_variance)

# Median Absolute Error (MedAE)
medae = median_absolute_error(y_test, y_pred_HuberReg)
print("Median Absolute Error for Huber Regressor:", medae)

"""# Least Angle Regresion"""

# For least Angle Regresion
from sklearn.linear_model import Lars

lars = Lars()
lars.fit(X_train, y_train)

# Make predictions on the test set.
y_pred_LARS = lars.predict(X_test)

# Evaluate the model's performance.
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error, explained_variance_score, median_absolute_error
# Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred_LARS)
print("Mean Squared Error for Least Angle Regressor:", mse)

# Root Mean Squared Error (RMSE)
rmse = mean_squared_error(y_test, y_pred_LARS, squared=False)
print("Root Mean Squared Error for Least Angle Regressor:", rmse)

# Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y_pred_LARS)
print("Mean Absolute Error for Least Angel Regressor:", mae)

# R-squared (R2)
r2 = r2_score(y_test, y_pred_LARS)
print("R-squared for Huber Regressor:", r2)

# Mean Absolute Percentage Error (MAPE)
mape = mean_absolute_percentage_error(y_test, y_pred_LARS)
print("Mean Absolute Percentage Error for Least Angel Regressor:", mape)

# Explained Variance Score
explained_variance = explained_variance_score(y_test, y_pred_LARS)
print("Explained Variance Score for Least Angel Regressor:", explained_variance)

# Median Absolute Error (MedAE)
medae = median_absolute_error(y_test, y_pred_LARS)
print("Median Absolute Error for Least Angel Regressor:", medae)

"""# LASSO REGRESSION MODEL"""

from sklearn.linear_model import Lasso

# Initialize the Lasso regression model.
lasso = Lasso(alpha=1.0)  # You can adjust the regularization strength (alpha) as needed.

# Train the Lasso model on the training data.
lasso.fit(X_train, y_train)

# Make predictions on the test set.
y_pred_lasso = lasso.predict(X_test)

# Evaluate the model's performance.
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error, explained_variance_score, median_absolute_error
# Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred_lasso)
print("Mean Squared Error for Least Angle Regressor:", mse)

# Root Mean Squared Error (RMSE)
rmse = mean_squared_error(y_test, y_pred_LARS, squared=False)
print("Root Mean Squared Error for Lasso Regression:", rmse)

# Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y_pred_lasso)
print("Mean Absolute Error for Lasso Regression:", mae)

# R-squared (R2)
r2 = r2_score(y_test, y_pred_lasso)
print("R-squared for Lasso Regression:", r2)

# Mean Absolute Percentage Error (MAPE)
mape = mean_absolute_percentage_error(y_test, y_pred_lasso)
print("Mean Absolute Percentage Error for Lasso Regression:", mape)

# Explained Variance Score
explained_variance = explained_variance_score(y_test, y_pred_lasso)
print("Explained Variance Score for Lasso Regresion:", explained_variance)

# Median Absolute Error (MedAE)
medae = median_absolute_error(y_test, y_pred_lasso)
print("Median Absolute Error for Lasso Regression:", medae)

"""# LASSOLARS MODEL"""

from sklearn.linear_model import LassoLars

# Initialize the LassoLars regression model.
lasso_lars = LassoLars(alpha=1.0)  # You can adjust the regularization strength (alpha) as needed.

# Train the LassoLars model on the training data.
lasso_lars.fit(X_train, y_train)

# Make predictions on the test set.
y_pred_lasso_lars = lasso_lars.predict(X_test)

# Evaluate the model's performance.
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error, explained_variance_score, median_absolute_error
# Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred_lasso_lars)
print("Mean Squared Error for Lasso Least Angle Regressor:", mse)

# Root Mean Squared Error (RMSE)
rmse = mean_squared_error(y_test, y_pred_lasso_lars, squared=False)
print("Root Mean Squared Error for Lasso Least Angle Regression:", rmse)

# Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y_pred_lasso_lars)
print("Mean Absolute Error for Lasso Least Angel Regression:", mae)

# R-squared (R2)
r2 = r2_score(y_test, y_pred_lasso_lars)
print("R-squared for Lasso Least Angel Regression:", r2)

# Mean Absolute Percentage Error (MAPE)
mape = mean_absolute_percentage_error(y_test, y_pred_lasso_lars)
print("Mean Absolute Percentage Error for Lasso Least Angel Regression:", mape)

# Explained Variance Score
explained_variance = explained_variance_score(y_test, y_pred_lasso_lars)
print("Explained Variance Score for Lasso Least Regresion:", explained_variance)

# Median Absolute Error (MedAE)
medae = median_absolute_error(y_test, y_pred_lasso_lars)
print("Median Absolute Error for Lasso Least Angle Regression:", medae)

# Extract the selected features and their coefficients.
selected_features_lasso_lars = X.columns[lasso_lars.coef_ != 0]
coefficients_lasso_lars = lasso_lars.coef_[lasso_lars.coef_ != 0]
print("Selected Features (LassoLars):", selected_features_lasso_lars)
print("Coefficients (LassoLars):", coefficients_lasso_lars)

# Vary Alpha and Path Visualization
import numpy as np
import matplotlib.pyplot as plt

alphas = np.logspace(-4, 4, 100)  # Vary alpha values over a range
coefs = []

for alpha in alphas:
    lasso_lars = LassoLars(alpha=alpha)
    lasso_lars.fit(X_train, y_train)
    coefs.append(lasso_lars.coef_)

# Plot the path of coefficients vs. alpha
plt.figure(figsize=(10, 6))
plt.plot(alphas, coefs)
plt.xscale('log')
plt.xlabel('Alpha (Regularization Strength)')
plt.ylabel('Coefficient Values')
plt.title('LassoLars Coefficient Path')
plt.axis('tight')
plt.show()

"""# Dummy Regression"""

from sklearn.dummy import DummyRegressor

# Initialize the Dummy Regressor with the "mean" strategy.
dummy_regressor = DummyRegressor(strategy="mean")

# Train the Dummy Regressor (no actual training is needed for this strategy).
dummy_regressor.fit(X_train, y_train)

# Make predictions using the Dummy Regressor.
y_pred_dummy = dummy_regressor.predict(X_test)

# Evaluate the model's performance.
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error, explained_variance_score, median_absolute_error
# Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred_dummy)
print("Mean Squared Error for Dummy Regressor:", mse)

# Root Mean Squared Error (RMSE)
rmse = mean_squared_error(y_test, y_pred_dummy, squared=False)
print("Root Mean Squared Error for Dummy Regressor:", rmse)

# Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y_pred_dummy)
print("Mean Absolute Error for Dummy Regressor:", mae)

# R-squared (R2)
r2 = r2_score(y_test, y_pred_dummy)
print("R-squared for Dummy Regressor:", r2)

# Mean Absolute Percentage Error (MAPE)
mape = mean_absolute_percentage_error(y_test, y_pred_dummy)
print("Mean Absolute Percentage Error for Dummy Regressor", mape)

# Explained Variance Score
explained_variance = explained_variance_score(y_test, y_pred_dummy)
print("Explained Variance Score for Dummy Regressor:", explained_variance)

# Median Absolute Error (MedAE)
medae = median_absolute_error(y_test, y_pred_dummy)
print("Median Absolute Error for Dummy Regressor:", medae)

"""# Ridge Regression"""

from sklearn.linear_model import Ridge

# Initialize the Ridge regression model with a specific alpha value.
ridge = Ridge(alpha=1.0)  # You can adjust the alpha value as needed.

# Train the Ridge model on the training data.
ridge.fit(X_train, y_train)

# Make predictions using the Ridge model.
y_pred_ridge = ridge.predict(X_test)

# Evaluate the model's performance.
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error, explained_variance_score, median_absolute_error
# Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred_ridge)
print("Mean Squared Error for ridge Regressor:", mse)

# Root Mean Squared Error (RMSE)
rmse = mean_squared_error(y_test, y_pred_ridge, squared=False)
print("Root Mean Squared Error for Ridge Regressor:", rmse)

# Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y_pred_ridge)
print("Mean Absolute Error for Ridge Regressor:", mae)

# R-squared (R2)
r2 = r2_score(y_test, y_pred_ridge)
print("R-squared for Ridge Regressor:", r2)

# Mean Absolute Percentage Error (MAPE)
mape = mean_absolute_percentage_error(y_test, y_pred_ridge)
print("Mean Absolute Percentage Error for Ridge Regressor", mape)

# Explained Variance Score
explained_variance = explained_variance_score(y_test, y_pred_ridge)
print("Explained Variance Score for Ridge Regressor:", explained_variance)

# Median Absolute Error (MedAE)
medae = median_absolute_error(y_test, y_pred_ridge)
print("Median Absolute Error for Ridge Regressor:", medae)

"""# KNeighborsRegressor"""

from sklearn.neighbors import KNeighborsRegressor

# Initialize the K Neighbors Regressor with a specific number of neighbors (k).
k_neighbors_regressor = KNeighborsRegressor(n_neighbors=5)  # You can adjust 'n_neighbors' as needed.

k_neighbors_regressor.fit(X_train, y_train)

# Make predictions using the Ridge model.
y_pred_k_neighbors_regressor = k_neighbors_regressor.predict(X_test)

# Evaluate the model's performance.
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error, explained_variance_score, median_absolute_error
# Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred_k_neighbors_regressor)
print("Mean Squared Error for KNN:", mse)

# Root Mean Squared Error (RMSE)
rmse = mean_squared_error(y_test, y_pred_k_neighbors_regressor, squared=False)
print("Root Mean Squared Error for KNN:", rmse)

# Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y_pred_k_neighbors_regressor)
print("Mean Absolute Error for KNN:", mae)

# R-squared (R2)
r2 = r2_score(y_test, y_pred_k_neighbors_regressor)
print("R-squared for KNN:", r2)

# Mean Absolute Percentage Error (MAPE)
mape = mean_absolute_percentage_error(y_test, y_pred_k_neighbors_regressor)
print("Mean Absolute Percentage Error for KNN", mape)

# Explained Variance Score
explained_variance = explained_variance_score(y_test, y_pred_k_neighbors_regressor)
print("Explained Variance Score for KNN:", explained_variance)

# Median Absolute Error (MedAE)
medae = median_absolute_error(y_test, y_pred_k_neighbors_regressor)
print("Median Absolute Error for KNN:", medae)

from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsRegressor

param_grid = {
    'n_neighbors': [3, 5, 7, 9, 11],  # Example values for 'n_neighbors'
    # You can include other hyperparameters and their ranges as needed.
}

k_neighbors_regressor = KNeighborsRegressor()
grid_search = GridSearchCV(k_neighbors_regressor, param_grid, cv=5, scoring='neg_mean_squared_error')

grid_search.fit(X_train, y_train)

# access the best hyperparameters and the best model.
best_params = grid_search.best_params_
best_estimator = grid_search.best_estimator_

# Evaluate the best model
y_pred_best_KNN = best_estimator.predict(X_test)

# Evaluate the model's performance.
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error, explained_variance_score, median_absolute_error
# Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred_best_KNN)
print("Mean Squared Error for y_pred_best_KNN:", mse)

# Root Mean Squared Error (RMSE)
rmse = mean_squared_error(y_test, y_pred_best_KNN, squared=False)
print("Root Mean Squared Error for y_pred_best_KNN:", rmse)

# Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y_pred_best_KNN)
print("Mean Absolute Error for y_pred_best_KNN:", mae)

# R-squared (R2)
r2 = r2_score(y_test, y_pred_best_KNN)
print("R-squared for y_pred_best_KNN:", r2)

# Mean Absolute Percentage Error (MAPE)
mape = mean_absolute_percentage_error(y_test, y_pred_best_KNN)
print("Mean Absolute Percentage Error for y_pred_best_KNN", mape)

# Explained Variance Score
explained_variance = explained_variance_score(y_test, y_pred_best_KNN)
print("Explained Variance Score for y_pred_best_KNN:", explained_variance)

# Median Absolute Error (MedAE)
medae = median_absolute_error(y_test, y_pred_best_KNN)
print("Median Absolute Error for y_pred_best_KNN:", medae)

"""# ElasticNet"""

from sklearn.linear_model import ElasticNet

# Initialize the Elastic Net model with specific alpha and l1_ratio values.
elastic_net = ElasticNet(alpha=1.0, l1_ratio=0.5)  # You can adjust 'alpha' and 'l1_ratio' as needed.

# Train the Elastic Net model on the training data.
elastic_net.fit(X_train, y_train)

# Make predictions using the Elastic Net model.
y_pred_elastic_net = elastic_net.predict(X_test)

# Evaluate the model's performance.
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error, explained_variance_score, median_absolute_error
# Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred_elastic_net)
print("Mean Squared Error for Elastic Net:", mse)

# Root Mean Squared Error (RMSE)
rmse = mean_squared_error(y_test, y_pred_elastic_net, squared=False)
print("Root Mean Squared Error for Elastic Net:", rmse)

# Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y_pred_elastic_net)
print("Mean Absolute Error for Elastic Net:", mae)

# R-squared (R2)
r2 = r2_score(y_test, y_pred_elastic_net)
print("R-squared for Elastic Net:", r2)

# Mean Absolute Percentage Error (MAPE)
mape = mean_absolute_percentage_error(y_test, y_pred_elastic_net)
print("Mean Absolute Percentage Error for Elastic Net", mape)

# Explained Variance Score
explained_variance = explained_variance_score(y_test, y_pred_elastic_net)
print("Explained Variance Score for Elastic Net:", explained_variance)

# Median Absolute Error (MedAE)
medae = median_absolute_error(y_test, y_pred_elastic_net)
print("Median Absolute Error for Elastic Net", medae)

"""# Orthogonal Matching Pursuit Model"""

from sklearn.linear_model import OrthogonalMatchingPursuit

# Initialize the Orthogonal Matching Pursuit model with the desired number of features.
omp = OrthogonalMatchingPursuit(n_nonzero_coefs=2)  # Adjust the number of features as needed.

# Train the OMP model on the training data.
omp.fit(X_train, y_train)

# Make predictions using the Orthogonal Matching Pursuit model.
y_pred_omp = omp.predict(X_test)

# Evaluate the model's performance.
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error, explained_variance_score, median_absolute_error
# Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred_omp)
print("Mean Squared Error for Orthogonal Matching Pursuit:", mse)

# Root Mean Squared Error (RMSE)
rmse = mean_squared_error(y_test, y_pred_omp, squared=False)
print("Root Mean Squared Error for Orthogonal Matching Pursuit:", rmse)

# Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y_pred_omp)
print("Mean Absolute Error for Othogonal Matching Pursuit:", mae)

# R-squared (R2)
r2 = r2_score(y_test, y_pred_omp)
print("R-squared for Othogonal Matching Pursuit:", r2)

# Mean Absolute Percentage Error (MAPE)
mape = mean_absolute_percentage_error(y_test, y_pred_omp)
print("Mean Absolute Percentage Error for Othogonal Matching Pursuit", mape)

# Explained Variance Score
explained_variance = explained_variance_score(y_test, y_pred_omp)
print("Explained Variance Score for orthogonal Matchiing Pursuit:", explained_variance)

# Median Absolute Error (MedAE)
medae = median_absolute_error(y_test, y_pred_omp)
print("Median Absolute Error for Orthogonal Matching Pursuit", medae)

"""# Light Gradient Boosting Machine"""

!pip install LightGBM

import lightgbm as lgb

# Define Hyperparameters:

# You need to specify the hyperparameters for your LightGBM model. Hyperparameter tuning can be an iterative process, so you
# might start with some reasonable values and then fine-tune as needed. Here's an example set of hyperparameters:

params = {
    'objective': 'regression',  # Set the objective for regression
    'boosting_type': 'gbdt',   # Gradient Boosting Decision Tree
    'metric': 'mse',           # Mean Squared Error
    'num_leaves': 31,          # Number of leaves in each tree
    'learning_rate': 0.05,     # Learning rate
    'feature_fraction': 0.9,   # Feature fraction for building trees
    'bagging_fraction': 0.8,   # Bagging fraction for sampling data
    'bagging_freq': 5,         # Frequency for bagging
    'verbose': 0               # Control the level of verbosity (0 means no messages)
}

# Create LightGBM datasets
lgb_train = lgb.Dataset(X_train, y_train)
lgb_test = lgb.Dataset(X_test, y_test, reference=lgb_train)

# Train the model without early stopping
num_boost_round = 1000  # Set the number of boosting rounds
model = lgb.train(params, lgb_train, num_boost_round=num_boost_round)

# Make predictions on the test set
y_pred_LightGBM = model.predict(X_test)

# Evaluate the model's performance.
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error, explained_variance_score, median_absolute_error
# Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred_LightGBM)
print("Mean Squared Error for LightGBM:", mse)

# Root Mean Squared Error (RMSE)
rmse = mean_squared_error(y_test, y_pred_LightGBM, squared=False)
print("Root Mean Squared Error for LightGBM:", rmse)

# Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y_pred_LightGBM)
print("Mean Absolute Error for y_pred_LightGBM:", mae)

# R-squared (R2)
r2 = r2_score(y_test, y_pred_LightGBM)
print("R-squared for LightGBM:", r2)

# Mean Absolute Percentage Error (MAPE)
mape = mean_absolute_percentage_error(y_test, y_pred_LightGBM)
print("Mean Absolute Percentage Error for LightGBM", mape)

# Explained Variance Score
explained_variance = explained_variance_score(y_test, y_pred_LightGBM)
print("Explained Variance Score for LightGBM:", explained_variance)

# Median Absolute Error (MedAE)
medae = median_absolute_error(y_test, y_pred_LightGBM)
print("Median Absolute Error for LightGBM", medae)

"""# Bayesian Ridge Regression"""

# First, import the Bayesian Ridge regression model from scikit-learn
from sklearn.linear_model import BayesianRidge

# Initialize the Bayesian Ridge model.
bayesian_ridge = BayesianRidge()

# Train the Bayesian Ridge model on the training data.
bayesian_ridge.fit(X_train, y_train)

# Make predictions using the Bayesian Ridge model.
y_pred_bayesian_ridge = bayesian_ridge.predict(X_test)

# Evaluate the model's performance.
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error, explained_variance_score, median_absolute_error
# Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred_bayesian_ridge)
print("Mean Squared Error for Bayesian Ridge:", mse)

# Root Mean Squared Error (RMSE)
rmse = mean_squared_error(y_test, y_pred_bayesian_ridge, squared=False)
print("Root Mean Squared Error for Bayesian Ridge:", rmse)

# Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y_pred_bayesian_ridge)
print("Mean Absolute Error for Bayesian Ridge:", mae)

# R-squared (R2)
r2 = r2_score(y_test, y_pred_bayesian_ridge)
print("R-squared for Bayesian Ridge:", r2)

# Mean Absolute Percentage Error (MAPE)
mape = mean_absolute_percentage_error(y_test, y_pred_LightGBM)
print("Mean Absolute Percentage Error for Bayesian Ridge", mape)

# Explained Variance Score
explained_variance = explained_variance_score(y_test, y_pred_bayesian_ridge)
print("Explained Variance Score for Bayesian Ridge:", explained_variance)

# Median Absolute Error (MedAE)
medae = median_absolute_error(y_test, y_pred_bayesian_ridge)
print("Median Absolute Error for Bayesian Ridge", medae)

"""# Passive Aggressive Regressor"""

# Import PassiveAggressiveRegressor from scikit-learn:
from sklearn.linear_model import PassiveAggressiveRegressor

# Initialize the Passive Aggressive Regressor model.
passive_aggressive_regressor = PassiveAggressiveRegressor()

# Train the Passive Aggressive Regressor model on the training data.
passive_aggressive_regressor.fit(X_train, y_train)

# Make predictions using the Passive Aggressive Regressor model.
y_pred_passive_aggressive = passive_aggressive_regressor.predict(X_test)

# Evaluate the model's performance.
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error, explained_variance_score, median_absolute_error
# Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred_passive_aggressive)
print("Mean Squared Error for passive Aggressive:", mse)

# Root Mean Squared Error (RMSE)
rmse = mean_squared_error(y_test, y_pred_passive_aggressive, squared=False)
print("Root Mean Squared Error for Passive Aggressive:", rmse)

# Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y_pred_passive_aggressive)
print("Mean Absolute Error for Passive Aggressive:", mae)

# R-squared (R2)
r2 = r2_score(y_test, y_pred_passive_aggressive)
print("R-squared for Passive Aggressive:", r2)

# Mean Absolute Percentage Error (MAPE)
mape = mean_absolute_percentage_error(y_test, y_pred_passive_aggressive)
print("Mean Absolute Percentage Error for Passive Aggressive", mape)

# Explained Variance Score
explained_variance = explained_variance_score(y_test, y_pred_passive_aggressive)
print("Explained Variance Score for Pred_passive Aggressive:", explained_variance)

# Median Absolute Error (MedAE)
medae = median_absolute_error(y_test, y_pred_passive_aggressive)
print("Median Absolute Error for Passive Aggressive", medae)

"""# AdaBoost Regressor"""

# Import AdaBoostRegressor from scikit-learn:
from sklearn.ensemble import AdaBoostRegressor

# Initialize the AdaBoost Regressor model with a base regressor.
base_regressor = DecisionTreeRegressor(max_depth=4)  # Example base regressor; you can choose different options.
adaboost_regressor = AdaBoostRegressor(base_regressor, n_estimators=50, learning_rate=0.1)  # Adjust hyperparameters as needed.

# Train the AdaBoost Regressor model on the training data.
adaboost_regressor.fit(X_train, y_train)

# Make predictions using the AdaBoost Regressor model.
y_pred_adaboost = adaboost_regressor.predict(X_test)

# Evaluate the model's performance.
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error, explained_variance_score, median_absolute_error
# Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred_adaboost )
print("Mean Squared Error for Adaboost Regressor :", mse)

# Root Mean Squared Error (RMSE)
rmse = mean_squared_error(y_test, y_pred_adaboost, squared=False)
print("Root Mean Squared Error for Adaboost:", rmse)

# Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y_pred_adaboost )
print("Mean Absolute Error for Passive Aggressive:", mae)

# R-squared (R2)
r2 = r2_score(y_test, y_pred_passive_aggressive)
print("R-squared for Adaboost :", r2)

# Mean Absolute Percentage Error (MAPE)
mape = mean_absolute_percentage_error(y_test, y_pred_adaboost )
print("Mean Absolute Percentage Error for Adaboost", mape)

# Explained Variance Score
explained_variance = explained_variance_score(y_test, y_pred_adaboost)
print("Explained Variance Score for Adaboost", explained_variance)

# Median Absolute Error (MedAE)
medae = median_absolute_error(y_test, y_pred_adaboost )
print("Median Absolute Error for Adaboost ", medae)

"""# CatBoost Regressor"""

!pip install catboost

#Import the catboostregresor model
from catboost import CatBoostRegressor

# Initialize the CatBoost Regressor model with specific hyperparameters.
catboost_regressor = CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=6, loss_function='RMSE')

# Train the CatBoost Regressor model on the training data.
catboost_regressor.fit(X_train, y_train, eval_set=(X_test, y_test), early_stopping_rounds=100, verbose=100)

# Make predictions using the CatBoost Regressor model.
y_pred_catboost = catboost_regressor.predict(X_test)

# Evaluate the model's performance.
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error, explained_variance_score, median_absolute_error
# Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred_catboost)
print("Mean Squared Error for Catboost :", mse)

# Root Mean Squared Error (RMSE)
rmse = mean_squared_error(y_test, y_pred_catboost, squared=False)
print("Root Mean Squared Error for Catboost:", rmse)

# Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y_pred_adaboost )
print("Mean Absolute Error for Catboost:", mae)

# R-squared (R2)
r2 = r2_score(y_test, y_pred_catboost)
print("R-squared for Catboost:", r2)

# Mean Absolute Percentage Error (MAPE)
mape = mean_absolute_percentage_error(y_test, y_pred_catboost)
print("Mean Absolute Percentage Error for Catboost", mape)

# Explained Variance Score
explained_variance = explained_variance_score(y_test, y_pred_catboost)
print("Explained Variance Score for Catboost", explained_variance)

# Median Absolute Error (MedAE)
medae = median_absolute_error(y_test, y_pred_catboost)
print("Median Absolute Error for Catboost ", medae)

"""# Gradient Boosting"""

#import the GradientBoostingRegressor from scikit-learn:from sklearn.ensemble import GradientBoostingRegressor
from sklearn.ensemble import GradientBoostingRegressor

from sklearn.ensemble import GradientBoostingRegressor

# Initialize the Gradient Boosting Regressor model with specific hyperparameters.
gradient_boosting_regressor = GradientBoostingRegressor(
    n_estimators=100,  # Number of boosting stages (trees)
    learning_rate=0.1,  # Step size at each iteration
    max_depth=4,  # Maximum depth of each tree
)

# Train the Gradient Boosting Regressor model on the training data.
gradient_boosting_regressor.fit(X_train, y_train)

# Make predictions using the Gradient Boosting Regressor model.
y_pred_gradient_boosting = gradient_boosting_regressor.predict(X_test)

# Evaluate the model's performance.
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error, explained_variance_score, median_absolute_error
# Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred_gradient_boosting)
print("Mean Squared Error for Gradient_boosting:", mse)

# Root Mean Squared Error (RMSE)
rmse = mean_squared_error(y_test, y_pred_gradient_boosting, squared=False)
print("Root Mean Squared Error for Gradient_boosting:", rmse)

# Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y_pred_gradient_boosting)
print("Mean Absolute Error for Gradient_boosting:", mae)

# R-squared (R2)
r2 = r2_score(y_test, y_pred_gradient_boosting)
print("R-squared for Gradient_boosting:", r2)

# Mean Absolute Percentage Error (MAPE)
mape = mean_absolute_percentage_error(y_test, y_pred_gradient_boosting)
print("Mean Absolute Percentage Error for Gradient_boosting", mape)

# Explained Variance Score
explained_variance = explained_variance_score(y_test, y_pred_gradient_boosting)
print("Explained Variance Score for Gradient_boosting", explained_variance)

# Median Absolute Error (MedAE)
medae = median_absolute_error(y_test, y_pred_gradient_boosting)
print("Median Absolute Error for Gradient_boosting ", medae)

"""# Extra Trees Regressor
### The Extra Trees Regressor is an ensemble machine learning model that belongs to  the Random Forest family. It is designed for regression tasks and works by constructing multiple decision trees during training and averaging their predictions to make robust predictions.
"""

# First, import the ExtraTreesRegressor from scikit-learn:
from sklearn.ensemble import ExtraTreesRegressor

# Initialize the Extra Trees Regressor model with specific hyperparameters.
extra_trees_regressor = ExtraTreesRegressor(
    n_estimators=100,  # Number of decision trees in the ensemble
    max_depth=4,  # Maximum depth of each tree
    random_state=42,  # Seed for reproducibility
)

# Train the Extra Trees Regressor model on the training data.
extra_trees_regressor.fit(X_train, y_train)

# Make predictions using the Extra Trees Regressor model.
y_pred_extra_trees = extra_trees_regressor.predict(X_test)

# Evaluate the model's performance.
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error, explained_variance_score, median_absolute_error
# Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred_extra_trees)
print("Mean Squared Error for Extra trees:", mse)

# Root Mean Squared Error (RMSE)
rmse = mean_squared_error(y_test, y_pred_extra_trees, squared=False)
print("Root Mean Squared Error for Extra Trees:", rmse)

# Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y_pred_extra_trees)
print("Mean Absolute Error for Extra Trees:", mae)

# R-squared (R2)
r2 = r2_score(y_test, y_pred_extra_trees)
print("R-squared for Extra Trees:", r2)

# Mean Absolute Percentage Error (MAPE)
mape = mean_absolute_percentage_error(y_test, y_pred_extra_trees)
print("Mean Absolute Percentage Error for Extra Trees", mape)

# Explained Variance Score
explained_variance = explained_variance_score(y_test, y_pred_extra_trees)
print("Explained Variance Score for Extra Trees", explained_variance)

# Median Absolute Error (MedAE)
medae = median_absolute_error(y_test, y_pred_extra_trees)
print("Median Absolute Error for Extra Trees", medae)

"""# Extreme Gradient Boosting
###Extreme Gradient Boosting (XGBoost) is a popular gradient boosting library known for its speed and performance. t's a powerful tool for regression tasks, including modeling aspects of the menstrual cycle.
"""

!pip install xgboost

from xgboost import XGBRegressor

# Initialize the XGBoost Regressor model with specific hyperparameters.
xgb_regressor = XGBRegressor(
    n_estimators=100,  # Number of boosting rounds (trees)
    learning_rate=0.1,  # Step size at each iteration
    max_depth=4,  # Maximum depth of each tree
    objective='reg:squarederror'  # Objective function for regression
)

# Train the XGBoost Regressor model on the training data.
xgb_regressor.fit(X_train, y_train)

# Make predictions using the XGBoost Regressor model.
y_pred_xgb = xgb_regressor.predict(X_test)

# Evaluate the model's performance.
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error, explained_variance_score, median_absolute_error
# Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred_xgb)
print("Mean Squared Error for xgb:", mse)

# Root Mean Squared Error (RMSE)
rmse = mean_squared_error(y_test, y_pred_xgb, squared=False)
print("Root Mean Squared Error for xgb:", rmse)

# Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y_pred_xgb)
print("Mean Absolute Error for xgb:", mae)

# R-squared (R2)
r2 = r2_score(y_test, y_pred_xgb)
print("R-squared for xgb:", r2)

# Mean Absolute Percentage Error (MAPE)
mape = mean_absolute_percentage_error(y_test, y_pred_xgb)
print("Mean Absolute Percentage Error for xgb", mape)

# Explained Variance Score
explained_variance = explained_variance_score(y_test, y_pred_xgb)
print("Explained Variance Score for xgb", explained_variance)

# Median Absolute Error (MedAE)
medae = median_absolute_error(y_test, y_pred_xgb)
print("Median Absolute Error for xgb", medae)

"""# ARIMA MODEL"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler

# Load your dataset and select relevant features
ovulation_dataset = df[['CycleNumber', 'LengthofCycle', 'LengthofLutealPhase', 'TotalNumberofHighDays', 'TotalNumberofPeakDays', 'UnusualBleeding', 'PhasesBleeding', 'IntercourseInFertileWindow', 'Age', 'BMI', 'Method', 'EstimatedDayofOvulation']]

# Assuming 'EstimatedDayofOvulation' is the target variable
target_variable = 'EstimatedDayofOvulation'

import statsmodels.api as sm

p = 1  # Autoregressive order
d = 1  # Differencing order (if needed, typically set to 1)
q = 1  # Moving average order

unique_values = ovulation_dataset[target_variable].unique()
print(unique_values)

ovulation_dataset = ovulation_dataset[ovulation_dataset[target_variable] != ' ']

default_value = 0  # Set the default value to 0

ovulation_dataset[target_variable] = ovulation_dataset[target_variable].replace(' ', default_value).astype(float)

ovulation_dataset[target_variable] = ovulation_dataset[target_variable].astype(float)

# Define the order of the ARIMA model (p, d, q)
order = (p, d, q)  # You should set appropriate values for p, d, and q



# Fit ARIMA model
model = sm.tsa.ARIMA(ovulation_dataset[target_variable], order=order)
results = model.fit()

# Summary of the ARIMA model
print(results.summary())

results = model.fit()  # Fit the ARIMA model
# Make predictions for future time steps
forecast_steps = 1515  # Adjust as needed
forecast = results.forecast(steps=forecast_steps)

import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error, explained_variance_score

# Assuming you have actual values and ARIMA model predictions
actual_values = ovulation_dataset[target_variable]  # Replace 'TargetColumn' with your target variable
predicted_values = forecast

# Calculate evaluation metrics
mae = mean_absolute_error(actual_values, predicted_values)
mse = mean_squared_error(actual_values, predicted_values)
rmse = np.sqrt(mse)
r2 = r2_score(actual_values, predicted_values)

# Calculate MAPE with handling of zero values in actuals
mape = np.mean(np.abs((actual_values - predicted_values) / actual_values)) * 100

explained_variance = explained_variance_score(actual_values, predicted_values)

print(f"Mean Absolute Error (MAE): {mae}")
print(f"Mean Squared Error (MSE): {mse}")
print(f"Root Mean Squared Error (RMSE): {rmse}")
print(f"R-squared (R2): {r2}")
print(f"Mean Absolute Percentage Error (MAPE): {mape}%")
print(f"Explained Variance Score: {explained_variance}")

# Evaluate the model's performance.
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error, explained_variance_score, median_absolute_error
# Mean Squared Error (MSE)
mse = mean_squared_error(actual_values, predicted_values)
print("Mean Squared Error for ARIMA:", mse)

# Root Mean Squared Error (RMSE)
rmse = mean_squared_error(actual_values, predicted_values, squared=False)
print("Root Mean Squared Error for ARIMA:", rmse)

# Mean Absolute Error (MAE)
mae = mean_absolute_error(actual_values, predicted_values)
print("Mean Absolute Error for ARIMA:", mae)

# R-squared (R2)
r2 = r2_score(actual_values, predicted_values)
print("R-squared for ARIMA:", r2)

# Mean Absolute Percentage Error (MAPE)
mape = mean_absolute_percentage_error(actual_values, predicted_values)
print("Mean Absolute Percentage Error for ARIMA", mape)

# Explained Variance Score
explained_variance = explained_variance_score(actual_values, predicted_values)
print("Explained Variance Score for ARIMA", explained_variance)

# Median Absolute Error (MedAE)
medae = median_absolute_error(actual_values, predicted_values)
print("Median Absolute Error for ARIMA", medae)



# Visualize the predictions
plt.figure(figsize=(12, 6))
plt.plot(ovulation_dataset.index, ovulation_dataset[target_variable], label='Observed')
plt.plot(pd.date_range(start=ovulation_dataset.index[-1], periods=forecast_steps + 1, closed='right'), forecast, label='Forecast', color='red')
plt.xlabel('Date')
plt.ylabel(target_variable)
plt.title('Ovulation Day Forecast')
plt.legend()
plt.show()

"""# Long Short Term Memory Model"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

from sklearn.preprocessing import LabelEncoder

# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Loop through columns in X_train
for column in ovulation_dataset.columns:
    if ovulation_dataset[column].dtype == 'object':  # Check if the column contains strings
        ovulation_dataset[column] = label_encoder.fit_transform(ovulation_dataset[column].astype(str))

# Normalize the data
scaler = MinMaxScaler()
ovulation_dataset = pd.DataFrame(scaler.fit_transform(ovulation_dataset), columns=ovulation_dataset.columns)

# Prepare training data with sequences
X, y = [], []
look_back = 5  # Number of time steps to look back
for i in range(len(ovulation_dataset) - look_back):
    X.append(ovulation_dataset.iloc[i:i + look_back].values)
    y.append(ovulation_dataset.iloc[i + look_back][target_variable])

X, y = np.array(X), np.array(y)

# Split into train and test
train_size = int(len(ovulation_dataset) * 0.8)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

# Build and train the LSTM model
model = Sequential()
model.add(LSTM(units=50, activation='relu', input_shape=(look_back, len(ovulation_dataset.columns))))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mean_squared_error')

model.fit(X_train, y_train, epochs=50, batch_size=1)

# Evaluate the model on test data
test_predictions = model.predict(X_test)

# Evaluate the model's performance.
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error, explained_variance_score, median_absolute_error
# Mean Squared Error (MSE)
mse = mean_squared_error(y_test, test_predictions)
print("Mean Squared Error for lstm:", mse)

# Root Mean Squared Error (RMSE)
rmse = mean_squared_error(y_test, test_predictions, squared=False)
print("Root Mean Squared Error for lstm:", rmse)

# Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, test_predictions)
print("Mean Absolute Error for lstm:", mae)

# R-squared (R2)
r2 = r2_score(y_test, test_predictions)
print("R-squared for lstm:", r2)

# Mean Absolute Percentage Error (MAPE)
mape = mean_absolute_percentage_error(y_test, test_predictions)
print("Mean Absolute Percentage Error for lstm", mape)

# Explained Variance Score
explained_variance = explained_variance_score(y_test, test_predictions)
print("Explained Variance Score for lstm", explained_variance)

# Median Absolute Error (MedAE)
medae = median_absolute_error(y_test, test_predictions)
print("Median Absolute Error for lstm", medae)